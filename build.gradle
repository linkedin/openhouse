buildscript {
  repositories {
    maven {
      url "https://repo.maven.apache.org/maven2/"
      metadataSources {
        gradleMetadata()
        mavenPom()
        artifact()
      }
    }
    maven {
      url "https://plugins.gradle.org/m2/"
      metadataSources {
        gradleMetadata()
        mavenPom()
        artifact()
      }
    }
  }
  dependencies {
    classpath "com.diffplug.spotless:spotless-plugin-gradle:6.9.1"
    classpath "com.diffplug.spotless:spotless-lib:2.9.0"
    classpath "com.diffplug.spotless:spotless-lib-extra:2.9.0"
    classpath "com.github.spotbugs:com.github.spotbugs.gradle.plugin:4.8.0"
  }
}

ext {
  spring_web_version = "2.7.8"
  spark_version = "3.1.1"
  ok_http3_version = "4.11.0"
  junit_version = "5.11.0"
  iceberg_1_2_version = "1.2.0.6"
  iceberg_1_5_version = "1.5.2.7"
}

group = 'com.linkedin.openhouse'

subprojects {
  buildDir = "${rootProject.buildDir}/${project.name}"
  group = rootProject.group
  version = rootProject.version
}

allprojects {
  apply plugin: "com.diffplug.spotless"
  apply plugin: "com.github.spotbugs"

  group = 'com.linkedin.openhouse'

  repositories {
    maven {
      url "https://repo.maven.apache.org/maven2/"
      metadataSources {
        gradleMetadata()
        mavenPom()
        artifact()
      }
    }
  }

  def excludedProjects = [
      ':integrations:spark:spark-3.5:openhouse-spark-3.5-itest',
      ':apps:openhouse-spark-apps-1.5_2.12'
  ]

  if (!excludedProjects.contains(it.path)) {
    configurations.all {
      resolutionStrategy {
        force 'com.fasterxml.jackson:jackson-bom:2.13.4'
        force 'com.fasterxml.jackson.core:jackson-databind:2.13.4'
        force 'org.apache.orc:orc-core:1.8.3'
        force 'com.google.guava:guava:33.5.0-jre'
      }
    }
  }

  plugins.withType(JavaPlugin) {
    dependencies {
      testImplementation "org.assertj:assertj-core:3.24.2" //assertions library
      testImplementation "org.junit.jupiter:junit-jupiter-api:" + junit_version
      testImplementation "org.junit.jupiter:junit-jupiter-params:" + junit_version
      testImplementation "org.mockito:mockito-core:4.11.0"
      testRuntimeOnly "org.junit.jupiter:junit-jupiter-engine:" + junit_version
    }

    tasks.withType(Test).configureEach {
      useJUnitPlatform()

      testLogging {
        // use these options for analyzing results: --console=plain > output.log 2>&1
        events "passed", "skipped", "failed" //, "started"
        // showStandardStreams = true
      }

       beforeTest { descriptor ->
           logger.lifecycle("Running test: ${descriptor.className}.${descriptor.name}")
       }
    }

    spotless {
      enforceCheck = false
      java {
        target '**/*.java'
        targetExclude 'client/**', 'build/**'

        importOrder('', 'static ')
        removeUnusedImports()
        trimTrailingWhitespace()

        endWithNewline()
        googleJavaFormat('1.7')
      }
    }

    spotbugs {
      includeFilter = rootProject.file('gradle/spotbugs/spotbugsInclude.xml')
      excludeFilter = rootProject.file('gradle/spotbugs/spotbugsExclude.xml')
      ignoreFailures = true
    }

    spotbugsMain {
      reports {
        html {
          required = true
          outputLocation = file("$buildDir/reports/spotbugs/main/spotbugs.html")
        }
      }
    }

    spotbugsTest.enabled = false
  }

  afterEvaluate {
    for (def task in it.tasks) {
      if (task != rootProject.tasks.CopyGitHooksTask) {
        task.dependsOn rootProject.tasks.CopyGitHooksTask
      }
    }
  }
}

// Local Git Hooks cannot be shared, as .git directory is gitignore'd.
tasks.register('CopyGitHooksTask', Copy) {
  println 'Make the git hook available in .git/hooks directory.'
  from file('scripts/git-hooks')
  into file('.git/hooks/')
}

// =============================================================================
// Docker Build Prerequisites
// =============================================================================
// These tasks make the implicit JAR dependencies for Docker builds explicit.
// Run `./gradlew dockerPrereqs` before `docker compose build`.
//
// JAR Dependencies by Dockerfile:
//   tables-service.Dockerfile      -> :services:tables:bootJar
//   housetables-service.Dockerfile -> :services:housetables:bootJar
//   jobs-service.Dockerfile        -> :services:jobs:bootJar
//   jobs-scheduler.Dockerfile      -> :apps:openhouse-spark-apps_2.12:shadowJar (uber JAR)
//   spark-base-hadoop2.8.dockerfile ->
//       :integrations:spark:spark-3.1:openhouse-spark-runtime_2.12:shadowJar (uber JAR)
//       :apps:openhouse-spark-apps_2.12:shadowJar (uber JAR)
//       :scripts:java:tools:dummytokens:jar
//   spark-3.5-base-hadoop3.2.dockerfile ->
//       :integrations:spark:spark-3.5:openhouse-spark-3.5-runtime_2.12:shadowJar (uber JAR)
//       :apps:openhouse-spark-apps_2.12:shadowJar (uber JAR)
//       :scripts:java:tools:dummytokens:jar
// =============================================================================

tasks.register('dockerPrereqs') {
  description = 'Builds all JAR files required by Docker images'
  group = 'docker'

  // Service bootJars (Spring Boot fat JARs)
  dependsOn ':services:tables:bootJar'
  dependsOn ':services:housetables:bootJar'
  dependsOn ':services:jobs:bootJar'

  // Spark runtime uber JARs (shadowJar)
  dependsOn ':integrations:spark:spark-3.1:openhouse-spark-runtime_2.12:shadowJar'
  dependsOn ':integrations:spark:spark-3.5:openhouse-spark-3.5-runtime_2.12:shadowJar'

  // Spark apps uber JAR (shadowJar)
  dependsOn ':apps:openhouse-spark-apps_2.12:shadowJar'

  // Utility JAR
  dependsOn ':scripts:java:tools:dummytokens:jar'

  doLast {
    println ''
    println '============================================================'
    println 'Docker prerequisites built successfully!'
    println ''
    println 'JAR files created:'
    println '  build/tables/libs/tables.jar'
    println '  build/housetables/libs/housetables.jar'
    println '  build/jobs/libs/jobs.jar'
    println '  build/openhouse-spark-runtime_2.12/libs/openhouse-spark-runtime_2.12-uber.jar'
    println '  build/openhouse-spark-3.5-runtime_2.12/libs/openhouse-spark-3.5-runtime_2.12-uber.jar'
    println '  build/openhouse-spark-apps_2.12/libs/openhouse-spark-apps_2.12-uber.jar'
    println '  build/dummytokens/libs/dummytokens*.jar'
    println ''
    println 'Ready for: docker compose build'
    println '============================================================'
  }
}

tasks.register('dockerBuild', Exec) {
  description = 'Builds Docker images after ensuring all JAR prerequisites are built'
  group = 'docker'

  dependsOn 'dockerPrereqs'

  def recipe = project.hasProperty('recipe') ? project.property('recipe') : 'oh-hadoop-spark'
  def recipeDir = "${rootDir}/infra/recipes/docker-compose/${recipe}"

  workingDir recipeDir
  commandLine 'docker', 'compose', 'build'

  doFirst {
    if (!file(recipeDir).exists()) {
      throw new GradleException("Recipe directory not found: ${recipeDir}\n" +
          "Available recipes: " + file("${rootDir}/infra/recipes/docker-compose")
              .listFiles()
              .findAll { it.isDirectory() && it.name != 'common' }
              .collect { it.name }
              .join(', '))
    }
    println ''
    println "Building Docker images for recipe: ${recipe}"
    println "Recipe directory: ${recipeDir}"
    println ''
  }
}

tasks.register('dockerUp', Exec) {
  description = 'Builds JARs, Docker images, and starts containers'
  group = 'docker'

  dependsOn 'dockerBuild'

  def recipe = project.hasProperty('recipe') ? project.property('recipe') : 'oh-hadoop-spark'
  def recipeDir = "${rootDir}/infra/recipes/docker-compose/${recipe}"

  workingDir recipeDir
  commandLine 'docker', 'compose', 'up', '-d'

  doFirst {
    println ''
    println "Starting containers for recipe: ${recipe}"
    println ''
  }

  doLast {
    println ''
    println '============================================================'
    println 'OpenHouse containers started successfully!'
    println ''
    println 'Services available at:'
    println '  Tables Service:      http://localhost:8000'
    println '  HouseTables Service: http://localhost:8001'
    println '  Jobs Service:        http://localhost:8002'
    println '  Prometheus:          http://localhost:9090'
    println ''
    println 'To stop: ./gradlew dockerDown -Precipe=' + recipe
    println '============================================================'
  }
}

tasks.register('dockerDown', Exec) {
  description = 'Stops and removes Docker containers'
  group = 'docker'

  def recipe = project.hasProperty('recipe') ? project.property('recipe') : 'oh-hadoop-spark'
  def recipeDir = "${rootDir}/infra/recipes/docker-compose/${recipe}"

  workingDir recipeDir
  commandLine 'docker', 'compose', 'down'

  doFirst {
    if (!file(recipeDir).exists()) {
      throw new GradleException("Recipe directory not found: ${recipeDir}\n" +
          "Available recipes: " + file("${rootDir}/infra/recipes/docker-compose")
              .listFiles()
              .findAll { it.isDirectory() && it.name != 'common' }
              .collect { it.name }
              .join(', '))
    }
    println ''
    println "Stopping containers for recipe: ${recipe}"
    println ''
  }
}
