"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9533],{4011:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>O,contentTitle:()=>L,default:()=>N,frontMatter:()=>I,metadata:()=>T,toc:()=>E});var r=s(5893),a=s(1151),o=s(7294),t=s(512),i=s(2466),l=s(6550),c=s(469),d=s(1980),h=s(7392),u=s(12);function p(e){return o.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function m(e){const{values:n,children:s}=e;return(0,o.useMemo)((()=>{const e=n??function(e){return p(e).map((e=>{let{props:{value:n,label:s,attributes:r,default:a}}=e;return{value:n,label:s,attributes:r,default:a}}))}(s);return function(e){const n=(0,h.l)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,s])}function g(e){let{value:n,tabValues:s}=e;return s.some((e=>e.value===n))}function b(e){let{queryString:n=!1,groupId:s}=e;const r=(0,l.k6)(),a=function(e){let{queryString:n=!1,groupId:s}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!s)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return s??null}({queryString:n,groupId:s});return[(0,d._X)(a),(0,o.useCallback)((e=>{if(!a)return;const n=new URLSearchParams(r.location.search);n.set(a,e),r.replace({...r.location,search:n.toString()})}),[a,r])]}function x(e){const{defaultValue:n,queryString:s=!1,groupId:r}=e,a=m(e),[t,i]=(0,o.useState)((()=>function(e){let{defaultValue:n,tabValues:s}=e;if(0===s.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!g({value:n,tabValues:s}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${s.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const r=s.find((e=>e.default))??s[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:n,tabValues:a}))),[l,d]=b({queryString:s,groupId:r}),[h,p]=function(e){let{groupId:n}=e;const s=function(e){return e?`docusaurus.tab.${e}`:null}(n),[r,a]=(0,u.Nk)(s);return[r,(0,o.useCallback)((e=>{s&&a.set(e)}),[s,a])]}({groupId:r}),x=(()=>{const e=l??h;return g({value:e,tabValues:a})?e:null})();(0,c.Z)((()=>{x&&i(x)}),[x]);return{selectedValue:t,selectValue:(0,o.useCallback)((e=>{if(!g({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);i(e),d(e),p(e)}),[d,p,a]),tabValues:a}}var f=s(2389);const k={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function j(e){let{className:n,block:s,selectedValue:a,selectValue:o,tabValues:l}=e;const c=[],{blockElementScrollPositionUntilNextRender:d}=(0,i.o5)(),h=e=>{const n=e.currentTarget,s=c.indexOf(n),r=l[s].value;r!==a&&(d(n),o(r))},u=e=>{let n=null;switch(e.key){case"Enter":h(e);break;case"ArrowRight":{const s=c.indexOf(e.currentTarget)+1;n=c[s]??c[0];break}case"ArrowLeft":{const s=c.indexOf(e.currentTarget)-1;n=c[s]??c[c.length-1];break}}n?.focus()};return(0,r.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,t.Z)("tabs",{"tabs--block":s},n),children:l.map((e=>{let{value:n,label:s,attributes:o}=e;return(0,r.jsx)("li",{role:"tab",tabIndex:a===n?0:-1,"aria-selected":a===n,ref:e=>c.push(e),onKeyDown:u,onClick:h,...o,className:(0,t.Z)("tabs__item",k.tabItem,o?.className,{"tabs__item--active":a===n}),children:s??n},n)}))})}function v(e){let{lazy:n,children:s,selectedValue:a}=e;const t=(Array.isArray(s)?s:[s]).filter(Boolean);if(n){const e=t.find((e=>e.props.value===a));return e?(0,o.cloneElement)(e,{className:"margin-top--md"}):null}return(0,r.jsx)("div",{className:"margin-top--md",children:t.map(((e,n)=>(0,o.cloneElement)(e,{key:n,hidden:e.props.value!==a})))})}function S(e){const n=x(e);return(0,r.jsxs)("div",{className:(0,t.Z)("tabs-container",k.tabList),children:[(0,r.jsx)(j,{...e,...n}),(0,r.jsx)(v,{...e,...n})]})}function w(e){const n=(0,f.Z)();return(0,r.jsx)(S,{...e,children:p(e.children)},String(n))}const y={tabItem:"tabItem_Ymn6"};function q(e){let{children:n,hidden:s,className:a}=e;return(0,r.jsx)("div",{role:"tabpanel",className:(0,t.Z)(y.tabItem,a),hidden:s,children:n})}const I={title:"Getting Started",tags:["Spark","Hadoop","Docker","OpenHouse","Iceberg","Azure"],sidebar_position:2},L="OpenHouse with Spark & S3",T={id:"getting_started",title:"Getting Started",description:"In this guide, we will quickly set up a running environment and experiment with some simple SQL commands. Our",source:"@site/docs/getting_started.mdx",sourceDirName:".",slug:"/getting_started",permalink:"/docs/getting_started",draft:!1,unlisted:!1,tags:[{label:"Spark",permalink:"/docs/tags/spark"},{label:"Hadoop",permalink:"/docs/tags/hadoop"},{label:"Docker",permalink:"/docs/tags/docker"},{label:"OpenHouse",permalink:"/docs/tags/open-house"},{label:"Iceberg",permalink:"/docs/tags/iceberg"},{label:"Azure",permalink:"/docs/tags/azure"}],version:"current",sidebarPosition:2,frontMatter:{title:"Getting Started",tags:["Spark","Hadoop","Docker","OpenHouse","Iceberg","Azure"],sidebar_position:2},sidebar:"docsSidebar",previous:{title:"Overview",permalink:"/docs/intro"},next:{title:"User Guide",permalink:"/docs/category/user-guide"}},O={},E=[{value:"Prerequisites",id:"prerequisites",level:3},{value:"Create and write to OpenHouse Tables",id:"create-and-write-to-openhouse-tables",level:2},{value:"Get environment ready",id:"get-environment-ready",level:3},{value:"Run SQL commands",id:"run-sql-commands",level:3},{value:"Prerequisites",id:"prerequisites-1",level:3},{value:"Create and write to OpenHouse Tables",id:"create-and-write-to-openhouse-tables-1",level:2},{value:"Get environment ready",id:"get-environment-ready-1",level:3},{value:"Run SQL commands",id:"run-sql-commands-1",level:3},{value:"Prerequisites",id:"prerequisites-2",level:3},{value:"Deployment",id:"deployment",level:2},{value:"Step 1",id:"step-1",level:3},{value:"Step 2",id:"step-2",level:3},{value:"Step 3",id:"step-3",level:3},{value:"Step 4",id:"step-4",level:3},{value:"Step 5",id:"step-5",level:3},{value:"Run SQL commands",id:"run-sql-commands-2",level:3},{value:"(Optional) Control access to Tables",id:"optional-control-access-to-tables",level:2}];function C(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(w,{children:[(0,r.jsxs)(q,{value:"s3",label:"S3",default:!0,children:[(0,r.jsx)(n.h1,{id:"openhouse-with-spark--s3",children:"OpenHouse with Spark & S3"}),(0,r.jsxs)(n.p,{children:["In this guide, we will quickly set up a running environment and experiment with some simple SQL commands. Our\nenvironment will include all the core OpenHouse services such as ",(0,r.jsx)(n.a,{href:"/docs/intro#catalog-service",children:"Catalog Service"}),",\n",(0,r.jsx)(n.a,{href:"/docs/intro#house-table-service",children:"House Table service"})," and ",(0,r.jsx)(n.a,{href:"/docs/intro#control-plane-for-tables",children:"others"}),",\n",(0,r.jsx)(n.a,{href:"https://spark.apache.org/releases/spark-release-3-1-1.html",children:"a Spark 3.1 engine"})," and\nalso ",(0,r.jsx)(n.a,{href:"https://min.io/docs/minio/container/index.html",children:"MinIO S3 Instance"}),".\nIn this walkthrough, we will create some tables on OpenHouse, insert data in them and query the data.\nFor more information on various docker environments and how to set them up\nplease see the ",(0,r.jsx)(n.a,{href:"https://github.com/linkedin/openhouse/blob/main/SETUP.md",children:"SETUP.md"})," guide."]}),(0,r.jsx)(n.p,{children:"In the consecutive optional section, you can learn more about some simple GRANT REVOKE commands and how\nOpenHouse manages access control."}),(0,r.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.docker.com/get-docker/",children:"Docker CLI"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/docker/compose-cli/blob/main/INSTALL.md",children:"Docker Compose CLI"})}),"\n"]}),(0,r.jsx)(n.h2,{id:"create-and-write-to-openhouse-tables",children:"Create and write to OpenHouse Tables"}),(0,r.jsx)(n.h3,{id:"get-environment-ready",children:"Get environment ready"}),(0,r.jsxs)(n.p,{children:["First, clone ",(0,r.jsx)(n.a,{href:"https://github.com/linkedin/openhouse",children:"OpenHouse github repository"})," and\nrun ",(0,r.jsx)(n.code,{children:"./gradlew build"})," command at the root directory. After the command succeeds you should see ",(0,r.jsx)(n.code,{children:"BUILD SUCCESSFUL"}),"\nmessage."]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"openhouse$main>  ./gradlew build\n"})}),(0,r.jsxs)(n.p,{children:["Execute ",(0,r.jsx)(n.code,{children:"docker compose -f infra/recipes/docker-compose/oh-s3-spark/docker-compose.yml up -d --build"})," command to\nbring up docker containers for OpenHouse services, Spark and S3."]}),(0,r.jsx)(n.h3,{id:"run-sql-commands",children:"Run SQL commands"}),(0,r.jsx)(n.p,{children:"Let us execute some basic SQL commands to create table, add data and query data."}),(0,r.jsx)(n.p,{children:"First login to the driver node and start the spark-shell."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"oh-hadoop-spark$main>  docker exec -it local.spark-master /bin/bash\n\nopenhouse@0a9ed5853291:/opt/spark$  bin/spark-shell --packages org.apache.iceberg:iceberg-spark-runtime-3.1_2.12:1.2.0,software.amazon.awssdk:bundle:2.20.18,software.amazon.awssdk:url-connection-client:2.20.18   \\\n--jars openhouse-spark-runtime_2.12-*-all.jar  \\\n--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,com.linkedin.openhouse.spark.extensions.OpenhouseSparkSessionExtensions   \\\n--conf spark.sql.catalog.openhouse=org.apache.iceberg.spark.SparkCatalog   \\\n--conf spark.sql.catalog.openhouse.catalog-impl=com.linkedin.openhouse.spark.OpenHouseCatalog     \\\n--conf spark.sql.catalog.openhouse.io-impl=org.apache.iceberg.aws.s3.S3FileIO   \\\n--conf spark.sql.catalog.openhouse.s3.endpoint=http://minioS3:9000  \\\n--conf spark.sql.catalog.openhouse.s3.access-key-id=admin  \\\n--conf spark.sql.catalog.openhouse.s3.secret-access-key=password  \\\n--conf spark.sql.catalog.openhouse.s3.path-style-access=true  \\\n--conf spark.sql.catalog.openhouse.metrics-reporter-impl=com.linkedin.openhouse.javaclient.OpenHouseMetricsReporter    \\\n--conf spark.sql.catalog.openhouse.uri=http://openhouse-tables:8080   \\\n--conf spark.sql.catalog.openhouse.auth-token=$(cat /var/config/openhouse.token) \\\n--conf spark.sql.catalog.openhouse.cluster=LocalS3Cluster\n"})}),(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["the configuration ",(0,r.jsx)(n.code,{children:"spark.sql.catalog.openhouse.uri=http://openhouse-tables:8080"})," points to the docker container\nrunning the ",(0,r.jsx)(n.a,{href:"/docs/intro#catalog-service",children:"OpenHouse Catalog Service"}),"."]})}),(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["the configuration ",(0,r.jsx)(n.code,{children:"spark.sql.catalog.openhouse.io-impl"})," is set to ",(0,r.jsx)(n.code,{children:"org.apache.iceberg.aws.s3.S3FileIO"})," in order\nenable IO operations on S3. Parameters for this connection is configured via the prefix ",(0,r.jsx)(n.code,{children:"spark.sql.catalog.openhouse.s3.*"}),"."]})}),(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["you can access the MinIO UI at ",(0,r.jsx)(n.code,{children:"http://localhost:9871"})," of your host machine and inspect the state of objects\ncreated for your table. The username is ",(0,r.jsx)(n.code,{children:"admin"})," and password is ",(0,r.jsx)(n.code,{children:"password"})," for the MinIO docker setup."]})})]}),(0,r.jsxs)(q,{value:"hdfs",label:"HDFS",children:[(0,r.jsx)(n.h1,{id:"openhouse-with-spark--hdfs",children:"OpenHouse with Spark & HDFS"}),(0,r.jsxs)(n.p,{children:["In this guide, we will quickly set up a running environment and experiment with some simple SQL commands. Our\nenvironment will include all the core OpenHouse services such as ",(0,r.jsx)(n.a,{href:"/docs/intro#catalog-service",children:"Catalog Service"}),",\n",(0,r.jsx)(n.a,{href:"/docs/intro#house-table-service",children:"House Table service"})," and ",(0,r.jsx)(n.a,{href:"/docs/intro#control-plane-for-tables",children:"others"}),",\n",(0,r.jsx)(n.a,{href:"https://spark.apache.org/releases/spark-release-3-1-1.html",children:"a Spark 3.1 engine"})," and\nalso ",(0,r.jsx)(n.a,{href:"https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html#NameNode+and+DataNodes",children:"HDFS namenode and datanode"}),".\nIn this walkthrough, we will create some tables on OpenHouse, insert data in them and query the data.\nFor more information on various docker environments and how to set them up\nplease see the ",(0,r.jsx)(n.a,{href:"https://github.com/linkedin/openhouse/blob/main/SETUP.md",children:"SETUP.md"})," guide."]}),(0,r.jsx)(n.p,{children:"In the consecutive optional section, you can learn more about some simple GRANT REVOKE commands and how\nOpenHouse manages access control."}),(0,r.jsx)(n.h3,{id:"prerequisites-1",children:"Prerequisites"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.docker.com/get-docker/",children:"Docker CLI"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/docker/compose-cli/blob/main/INSTALL.md",children:"Docker Compose CLI"})}),"\n"]}),(0,r.jsx)(n.h2,{id:"create-and-write-to-openhouse-tables-1",children:"Create and write to OpenHouse Tables"}),(0,r.jsx)(n.h3,{id:"get-environment-ready-1",children:"Get environment ready"}),(0,r.jsxs)(n.p,{children:["First, clone ",(0,r.jsx)(n.a,{href:"https://github.com/linkedin/openhouse",children:"OpenHouse github repository"})," and\nrun ",(0,r.jsx)(n.code,{children:"./gradlew build"})," command at the root directory. After the command succeeds you should see ",(0,r.jsx)(n.code,{children:"BUILD SUCCESSFUL"}),"\nmessage."]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"openhouse$main>  ./gradlew build\n"})}),(0,r.jsxs)(n.p,{children:["Execute ",(0,r.jsx)(n.code,{children:"docker compose -f infra/recipes/docker-compose/oh-hadoop-spark/docker-compose.yml up -d --build"})," command to\nbring up docker containers for OpenHouse services, Spark and HDFS."]}),(0,r.jsx)(n.h3,{id:"run-sql-commands-1",children:"Run SQL commands"}),(0,r.jsx)(n.p,{children:"Let us execute some basic SQL commands to create table, add data and query data."}),(0,r.jsx)(n.p,{children:"First login to the driver node and start the spark-shell."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"oh-hadoop-spark$main>  docker exec -it local.spark-master /bin/bash\n\nopenhouse@0a9ed5853291:/opt/spark$  bin/spark-shell --packages org.apache.iceberg:iceberg-spark-runtime-3.1_2.12:1.2.0   \\\n--jars openhouse-spark-runtime_2.12-*-all.jar  \\\n--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,com.linkedin.openhouse.spark.extensions.OpenhouseSparkSessionExtensions   \\\n--conf spark.sql.catalog.openhouse=org.apache.iceberg.spark.SparkCatalog   \\\n--conf spark.sql.catalog.openhouse.catalog-impl=com.linkedin.openhouse.spark.OpenHouseCatalog     \\\n--conf spark.sql.catalog.openhouse.metrics-reporter-impl=com.linkedin.openhouse.javaclient.OpenHouseMetricsReporter    \\\n--conf spark.sql.catalog.openhouse.uri=http://openhouse-tables:8080   \\\n--conf spark.sql.catalog.openhouse.auth-token=$(cat /var/config/openhouse.token) \\\n--conf spark.sql.catalog.openhouse.cluster=LocalHadoopCluster\n"})}),(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["the configuration ",(0,r.jsx)(n.code,{children:"spark.sql.catalog.openhouse.uri=http://openhouse-tables:8080"})," points to the docker container\nrunning the ",(0,r.jsx)(n.a,{href:"/docs/intro#catalog-service",children:"OpenHouse Catalog Service"}),"."]})})]}),(0,r.jsxs)(q,{value:"azure",label:"Azure",children:[(0,r.jsx)(n.h1,{id:"openhouse-with-spark--azure",children:"OpenHouse with Spark & Azure"}),(0,r.jsxs)(n.p,{children:["In this guide, we will set up a running sandbox environment and experiment with some simple SQL commands. Our\nenvironment includes the both ",(0,r.jsx)(n.a,{href:"/docs/intro#catalog-service",children:"Catalog Service"})," and ",(0,r.jsx)(n.a,{href:"/docs/intro#house-table-service",children:"House Table Service"}),".\nIt also utilizes ",(0,r.jsx)(n.a,{href:"https://spark.apache.org/releases/spark-release-3-1-1.html",children:"a Spark 3.1 engine"})," and\nAzure services provisioned with ",(0,r.jsx)(n.a,{href:"https://developer.hashicorp.com/terraform?ajs_aid=a460286e-8e8f-4072-aa29-09a063308053&product_intent=terraform",children:"Terraform"}),".\nIn this walkthrough, we will spin up this sandbox environment and deploy the OpenHouse services in Azure. Then, we will\ncreate some tables, insert data in them, and query the data. For more information on the sandbox setup, checkout the\n",(0,r.jsx)(n.a,{href:"https://github.com/linkedin/openhouse/blob/main/infra/recipes/terraform/azure/environments/sandbox/README.md",children:"README.md"})," file."]}),(0,r.jsx)(n.p,{children:"In the consecutive optional section, you can learn more about some simple GRANT REVOKE commands and how\nOpenHouse manages access control."}),(0,r.jsx)(n.h3,{id:"prerequisites-2",children:"Prerequisites"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.docker.com/get-docker/",children:"Docker CLI"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/docker/compose-cli/blob/main/INSTALL.md",children:"Docker Compose CLI"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/cli/azure/",children:"Azure CLI"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli",children:"Terraform CLI"})}),"\n"]}),(0,r.jsx)(n.h2,{id:"deployment",children:"Deployment"}),(0,r.jsx)(n.h3,{id:"step-1",children:"Step 1"}),(0,r.jsxs)(n.p,{children:["Clone ",(0,r.jsx)(n.a,{href:"https://github.com/linkedin/openhouse",children:"OpenHouse github repository"})," and\nrun ",(0,r.jsx)(n.code,{children:"./gradlew build"})," command at the root directory. After the command succeeds you should see ",(0,r.jsx)(n.code,{children:"BUILD SUCCESSFUL"}),"\nmessage."]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"openhouse$main>  ./gradlew build\n"})}),(0,r.jsx)(n.h3,{id:"step-2",children:"Step 2"}),(0,r.jsxs)(n.p,{children:["Setup an Azure account and create/choose a subscription. Run ",(0,r.jsx)(n.code,{children:"az login"})," to login to your Azure account and subscription from the terminal."]}),(0,r.jsx)(n.h3,{id:"step-3",children:"Step 3"}),(0,r.jsx)(n.p,{children:"Navigate into the container environment, and initialize and apply the Terraform configuration:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"cd infra/recipes/terraform/azure/environments/container\nterraform init\nterraform apply\n"})}),(0,r.jsx)(n.h3,{id:"step-4",children:"Step 4"}),(0,r.jsx)(n.p,{children:"Navigate into the sandbox environment, and initialize and apply the Terraform configuration:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"cd infra/recipes/terraform/azure/environments/sandbox\nterraform init\nterraform apply\n"})}),(0,r.jsx)(n.h3,{id:"step-5",children:"Step 5"}),(0,r.jsx)(n.p,{children:"Start the docker containers for the Spark shell:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"docker compose -f infra/recipes/docker-compose/spark-only/docker-compose.yml up -d --build\n"})}),(0,r.jsx)(n.h3,{id:"run-sql-commands-2",children:"Run SQL commands"}),(0,r.jsx)(n.p,{children:"Start the Spark shell by running"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"./scripts/spark-shell.sh\n"})}),(0,r.jsx)(n.p,{children:'When prompted, type "Y".'})]})]}),"\n",(0,r.jsx)(n.p,{children:"Once the spark-shell is up, we run the following command to create a simple table."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:'scala>  spark.sql("CREATE TABLE openhouse.db.tb (ts timestamp, data string) PARTITIONED BY (days(ts))")\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Run a ",(0,r.jsx)(n.code,{children:"SHOW TABLES"})," command to confirm the table that we just created!"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:'\nscala> spark.sql("SHOW TABLES IN openhouse.db").show\n\n+---------+---------+\n|namespace|tableName|\n+---------+---------+\n| db      |       tb|\n+---------+---------+\n\n'})}),"\n",(0,r.jsx)(n.p,{children:"Great! We have created our first table. Now, let us put some data in it and retrieve it."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:'\nscala>  spark.sql("""\nINSERT INTO TABLE openhouse.db.tb VALUES\n   (current_timestamp(), \'today\'),\n   (date_sub(CAST(current_timestamp() as DATE), 30), \'today-30d\')\n """)\n\nscala> spark.sql("SELECT * FROM openhouse.db.tb").show\n\n+--------------------+---------+\n|                  ts|     data|\n+--------------------+---------+\n|2024-03-22 19:39:...|    today|\n| 2024-02-21 00:00:00|today-30d|\n+--------------------+---------+\n\n'})}),"\n",(0,r.jsx)(n.p,{children:"Looks great! We just added some data to OpenHouse and queried the data using Spark SQL."}),"\n",(0,r.jsxs)(n.p,{children:["To find out more about other SQL commands that OH supports, please visit the ",(0,r.jsx)(n.a,{href:"/docs/User%20Guide/Catalog/SQL",children:"SQL User Guide"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"optional-control-access-to-tables",children:"(Optional) Control access to Tables"}),"\n",(0,r.jsx)(n.p,{children:"We will continue with the same environment and the table (ie db.table) as before for this section."}),"\n",(0,r.jsxs)(n.p,{children:["You might have seen the parameter ",(0,r.jsx)(n.code,{children:"spark.sql.catalog.openhouse.auth-token=$(cat /var/config/openhouse.token)"})," when you\nlaunched the sparkshell. This parameter sets up the client with your user token."]}),"\n",(0,r.jsx)(n.p,{children:"As you did before, start the spark-shell and run the following SQL command to make it fail."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-SQL",children:'scala> spark.sql("GRANT SELECT ON TABLE openhouse.db.tb TO user_1").show\n\njava.lang.IllegalArgumentException: 400 , {"status":"BAD_REQUEST","error":"Bad Request","message":"db.tb2 is not a shared table","stacktrace":null,"cause":"Not Available"}\n'})}),"\n",(0,r.jsxs)(n.p,{children:["This error means the table is not sharable. ",(0,r.jsx)(n.strong,{children:"In OpenHouse, tables are private by default"}),". You can share them by\nrunning the SQL command:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-SQL",children:'scala> spark.sql("ALTER TABLE openhouse.db.tb SET POLICY ( SHARING=true )")\n'})}),"\n",(0,r.jsx)(n.p,{children:"In order to check the ACLs for this table, run:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-SQL",children:'\nscala> spark.sql("SHOW GRANTS ON TABLE openhouse.db.tb2").show\n+---------+---------+\n|privilege|principal|\n+---------+---------+\n|   SELECT|   user_1|\n+---------+---------+\n\n'})}),"\n",(0,r.jsxs)(n.p,{children:["You can also apply similar access control for database entity, please refer to the\n",(0,r.jsx)(n.a,{href:"/docs/User%20Guide/Catalog/SQL#grant-revoke",children:"User Guide"})," to learn more."]})]})}function N(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(C,{...e})}):C(e)}},1151:(e,n,s)=>{s.d(n,{Z:()=>i,a:()=>t});var r=s(7294);const a={},o=r.createContext(a);function t(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);