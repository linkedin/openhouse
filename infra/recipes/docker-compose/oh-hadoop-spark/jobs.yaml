jobs:
  defaults:
    &apps-defaults
      spark-properties: &spark-defaults {
        "spark.sql.extensions": "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,com.linkedin.openhouse.spark.extensions.OpenhouseSparkSessionExtensions",
        "spark.sql.catalog.openhouse": "org.apache.iceberg.spark.SparkCatalog",
        "spark.sql.catalog.openhouse.catalog-impl": "com.linkedin.openhouse.spark.OpenHouseCatalog",
        "spark.sql.catalog.openhouse.uri": "http://openhouse-tables:8080",
        "spark.sql.catalog.openhouse.cluster": "LocalHadoopCluster",
        "spark.jars.packages": "org.apache.iceberg:iceberg-spark-runtime-3.1_2.12:1.2.0",
        "spark.sql.autoBroadcastJoinThreshold": "-1",
        "spark.driver.memory": "2g"
      }
  spark:
    engines:
      - &livy-engine
        engine-type: LIVY
        coordinator-class-name: com.linkedin.openhouse.jobs.services.livy.LivyJobsCoordinator
        engine-uri: http://spark-livy:8998
        jar-path: local:/opt/spark/openhouse-spark-apps_2.12-latest-all.jar
        dependencies:
          - local:/opt/spark/openhouse-spark-runtime_2.12-latest-all.jar
    default-engine: LIVY
    storage-uri: http://openhouse-housetables:8080
    metrics-uri: http://localhost:4318
    auth-token-path: "/var/config/openhouse.token"
    apps:
      - type: NO_OP
        class-name: com.linkedin.openhouse.jobs.spark.NoOpSparkApp
        args: []
        << : *apps-defaults
        << : *livy-engine
      - type: SQL_TEST
        class-name: com.linkedin.openhouse.jobs.spark.OpenHouseCatalogSQLTestSparkApp
        args: []
        << : *apps-defaults
        << : *livy-engine
      - type: RETENTION
        class-name: com.linkedin.openhouse.jobs.spark.RetentionSparkApp
        args: []
        << : *apps-defaults
        << : *livy-engine
      - type: DATA_COMPACTION
        class-name: com.linkedin.openhouse.jobs.spark.DataCompactionSparkApp
        args: []
        << : *apps-defaults
        << : *livy-engine
      - type: SNAPSHOTS_EXPIRATION
        class-name: com.linkedin.openhouse.jobs.spark.SnapshotsExpirationSparkApp
        args: []
        << : *apps-defaults
        << : *livy-engine
      - type: ORPHAN_FILES_DELETION
        class-name: com.linkedin.openhouse.jobs.spark.OrphanFilesDeletionSparkApp
        args: []
        <<: *apps-defaults
        spark-properties:
          <<: *spark-defaults
          "spark.driver.memory": "2g"
        << : *livy-engine
      - type: STAGED_FILES_DELETION
        class-name: com.linkedin.openhouse.jobs.spark.StagedFilesDeletionSparkApp
        args: ["--trashDir", ".trash", "--daysOld", "10", "--recursive", "true"]
        <<: *apps-defaults
        << : *livy-engine
      - type: ORPHAN_DIRECTORY_DELETION
        class-name: com.linkedin.openhouse.jobs.spark.OrphanTableDirectoryDeletionSparkApp
        args: [ "--trashDir", ".trash" ]
        <<: *apps-defaults
        << : *livy-engine
      - type: TABLE_STATS_COLLECTION
        class-name: com.linkedin.openhouse.jobs.spark.TableStatsCollectionSparkApp
        args: []
        <<: *apps-defaults
        << : *livy-engine
      - type: DATA_LAYOUT_STRATEGY_GENERATION
        class-name: com.linkedin.openhouse.jobs.spark.DataLayoutStrategyGeneratorSparkApp
        args: ["--outputTableName", "u_openhouse.dlo_strategies", "--partitionLevelOutputTableName", "u_openhouse.dlo_partition_strategies"]
        <<: *apps-defaults
        << : *livy-engine
      - type: DATA_LAYOUT_STRATEGY_EXECUTION
        class-name: com.linkedin.openhouse.jobs.spark.DataCompactionSparkApp
        args: []
        <<: *apps-defaults
        << : *livy-engine
      - type: SORT_STATS_COLLECTION
        class-name: com.linkedin.openhouse.jobs.spark.SortStatsCollectionSparkApp
        args: []
        <<: *apps-defaults
        <<: *livy-engine

